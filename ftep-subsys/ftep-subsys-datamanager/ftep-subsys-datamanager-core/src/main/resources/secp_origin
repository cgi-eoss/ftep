#!/bin/env bash
# Project: secp
# Outline: secp is a generic utility to perform URL downloads to a sink directory.
#          the supported url schemes are ftp, scp, http, https, gridftp, file and nfs
#          when available in compressed form or packaged in an archive, the files/directories
#          are automatically decompressed/unpackaged to the sink name.
#          the following archive formats are recognised:
#          .gz, .tgz, .zip
#
#          as default, the software will follow:
#            * .uar text files containing a list of URLs (one per line) to download.
#            * HTML meta-refresh and href tags to interactively download files from the web.
#            * RDF online resources (<dclite4g:onlineResource> tag)
#
# Dependencies: curl, scp, gawk, sed, bash, globus-url-copy [optional]
#
# Change log:
# version 1.0
#       * first release
# version 1.1
#       * add the -f option to force a local copy of the file in case of nfs driver
#       * add the handling of the new return message of gridftp server "No such file or directory"
# version 1.2
#       * corrected mkdir with -p flag when creating output directory
# version 1.3
#       * changed the -R flag semantic for the opposite to retry on timeout by default
#
# version 2.0 2008-05-17
#       * added support for scp and for http, https and ftp via wget
#       * improved error handling in particular for nfs and gridftp drivers
#       * improved message logging to stderr
#       * use of logApp function to log messages if available, unless LOG_FUNCTION variable is defined
#       * add -q (quiet) option to suppress echo of local filenames to stdout
#       * add -O option to force file or directory overrides in output directory (default is to not override)
#       * add -w (work directory) option to specify working directory (defaulting to /tmp)
#       * add watchdog for secp hangs in particular for gridftp or wget transfers (-t and -R options)
#       * add unpacking support for .tar, .tgz, .tar.gz, .Z, tar.Z, .bz, .bz2, .tbz, .tar.bz, .tar.bz2, .zip
#       * add support for uar (url archive) type unpacking
#       * add -c -p and -b options 
#       * add -z option
# version 2.1 2008-07-16 by manu
#       * removed -fast option
# version 2.2
#       * corrected mkdir with -p flag when creating the tmp input file directory
#       * added the gsiftp driver (same of gridftp)
#       * added the cache driver
# version 2.3
#       * added https driver with curl (for gridsite support)
#       * removed http driver with GET (outdated)
#       * modified usage
#       * removed ams driver (outdated)
# version 2.3.1
#       * fixed error parsing for https driver
# version 2.4
#       * added automatic uncompression of .gz files. If you want to disable it, you need to add the -z option
#       * added s3 driver
#       * added s (skip) option
#
# version 3.0
#       * rewritten for performances (removed external log function support) - more than 10 times faster
#       * removed dependency on bash_debug.sh watchdog and log function
#       * added long opt support. Multiple options with one - is not supported anymore (ex. -co is not supported, shall be -c -o)
#       * removed -b, -Z, -w option. Not used anymore.
#       * -D option is deprecated. Debug can be now performed using standard bash debugging tools (sh -x)
#       * timeout (-t option) is now expressed in seconds
#       * secp now uncompress the files even if .gz or .tgz is written in the URI, since the new catalog
#         contains URIs with the .gz and .tgz suffix, this is needed for retro compatibility of the services.
#         This can be disabled with the new -U option. Moreover, for performance issues, secp will try adding only the
#         .gz extension if the file do not exist, and not all the others.
#       * added possibility to follow RDF and HTML auto-refresh meta-tag and HREF links for support to the new cache ws protocol
#         and for direct download from the G-POD catalogue. This can be disabled using the new -H option.
#       * removed support for un-compression of tar.gz, .Z, tar.Z, .bz, .bz2, .tbz, .tar.bz, .tar.bz2 files (not used anymore)
#       * retries (-r) and timeouts (-t) are now handled by the drivers (for performance issues)
#       * added -rt option to setup delay between retries
#       * removed WGET dependency, using curl insthead
# version 3.0.1
#       * added -w option (set-up tmp directory base for drivers)
#       * added -co -qo -qco for retro-compatibility.
# version 3.0.2
#       * unzip support for multiple files in the zip
# version 3.1
#       * merged with ciop-tool version 3.0.0
#       * added FILE driver support for directories copy (from ciop-tool, with -x switch to exclude files in the copy)
#       * added HDFS driver (from ciop-tool)
#       * added support for EO-SSO login followup and HTTP basic authentication
#       * added support for credeltials storing in the user home
#       * added support for session cookies (for cURL driver)
#       * fixed https proxy authentication for SL6
# version 3.1.1
#       * fixed minor bugs
#       * added -F for URL load from file list
# version 3.1.2
#       * fixed unzip folder detection plus other minor fixes
# version 3.1.3
#       * fixed EO-SSO support for test servers
#       * fixed minor bugs
#
# License:
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
SECP_VERSION=3.1.3

# Usage:
function usage {
cat <<:usage
secp version $SECP_VERSION
Usage:
secp  [-h] [-a] [-q] [-f] [-b <url-base>] [-d <driver-def>] 
      [-o|O <sink-dir>] [-w <work-dir>] [-c] [-p <prefix>] [-z|-Z]
      [-r <num-retries>] [-t <timeout>] [-R] [-l <log-label>] [-D]
      <url1> [<url2> ... <urlN>]

URL Parameters:
      Arguments are URL strings provided by seurl
      if a parameter is specified as '-', URLs are read and inserted from standard input
 
Options:
      -h               displays this help page
      -a               abort on first error without attempting to process further URLs
      -q               quiet mode, local filenames are not echoed to stdout after transfer
      -f               force transfer of a physical copy of the file in case of nfs URLs
      -F <url-list>    get URLs from the <url-list> file
      -d <driver-file> get additional drivers from shell file <driver-file>. Drivers shall contain
                       a named <protocol>Driver
      -o|O <out-dir>   defines the output directory for transfers (default is $PWD)
                       with -O the sink files or directories possibly existing in the output
                       directory will be overwritten.
      -c               creates the output directory if it does not exist
      -p <prefix>      prepend the given prefix to all output names
      -z               provide output as a compressed package (.gz for files or .tgz for folders). NOTE
                       that it will not compress already compressed files (.gz, .tgz or .zip)
      -U|--no-uzip     disable file automatic decompression of .gz, .tgz and .zip files.
      -r <num-retries> defines the maximum number of retries (default is 5)
      -rt <seconds>    define the time (in seconds) between retries (default is 60)
      -t <timeout>     defines the timer (in seconds) for the watchdog timeout applicable to
                       gridftp, scp, ftp, http, and https schemes (default is 600 seconds)
      -R               do not retry transfer after timeout
      -D               set debug mode for command output parsing debugging
      -H               do not follow html and rdf tags and .uar archives.
      -s               skip download if sink path already exists
      -w <tmpdir>      set up temporary directory for drivers (default to /tmp)
      -x <pattern>     exclude the files matching the pattern for directory input
      -K               private mode, disable storing of authentication session in ~/.secp_sess file
                       and passwords in the ~/.secp_cred file.
      -C <user>:<pass> force <user> and <pass> authentication (NOTE: these passwords will be stored
                       in clear text in the ~/.secp_cred file. NOTE: If username/passowrd is specified
                       it has precedence over the certificate/proxy authentication)

Output:
      unless the quiet option is used (-q), the local path of each file (or directory) 
      downloaded after each URL transfer is echoed, one per line
      unless the -U option is used, if the output file is a .gz or .tgz file it will be
      decompressed
      unless the -H options is specified, the software will follow the RDF <dclite4g:onlineResource>
      and the HTML href and refresh tags.
      the software will perform authentication if credentials are specified. Supported authentication
      types are 'basic', EO-SSO and session cookies. Credentials are stored in the ~/.secp_cred file,
      session cookies are stored in the ~/.secp_sess file (use -K switch to disable this behaviour)

Exit codes:
      0      all URLs were successfully downloaded
      1      an error occured during processing
      255    environment is invalid (e.g. invalid working directory) or invalid options are provided
      254    output directory does not exist or failed creating it (with -c option)

      if the -a option is used, the exit code is set to the error code of the last URL transfer:
      252    no driver available for URL
      251    an existing file or directory conflicts with the sink for the URL in the output directory
      250    an error occured while unpacking the output file or when packaging/compressing the output
             file (when -z or -Z option is used)
      128    a timeout occured while fetching an url
      127    a fatal error occured, source of error is not known or not handled by driver
      <128   error codes specific to the transfer scheme
      1      resource pointed by input URL do not exist
:usage
  exit 0
}

[[ -z "$1" ]] && usage

#Default parameters
_SKIP_URI_ERRORS=true
_ABORT_ON_URI_ERROR=false
_QUIET=false
_COPY=false
_RETRY=5
_RETRY_SLEEP=60
_CONNECTION_TIMEOUT=600
_COMPRESS_OUTPUT=false
_OUTPUT_PREFIX=
_CREATE_OUTPUT_DIR=false
_OUTPUT_DIR="$PWD"
_INPUT_URI=
_UNCOMPRESS=true
_FOLLOW_HTML=true
_OVERWRITE=false
_SKIP_EXISTING_SINKS=false
_TMP_DIR=/tmp
_DIR_EXCLUDE_PATTERN=
_USER_USERNAME=
_USER_PASSWORD=
_AUTH_DATA_FILE="$HOME/.secp_cred"
_SESSION_FILE="$HOME/.secp_sess"

#Parse arguments
while [[ "$#" -gt 0 ]]; do
  case "$1" in
   -a) _ABORT_ON_URI_ERROR=true; shift 1 ;;
   -s) _SKIP_EXISTING_SINKS=true; shift 1 ;;
   -q) _QUIET=true; shift 1 ;;
   -f) _COPY=true; shift 1 ;;
   -F) [[ -e "$2" ]] && _INPUT_URI="$_INPUT_URI `tr '\n' ' '<$2`"; shift 2 ;;
   -D) echo "[WARNING] -D option is deprecated. Launch this script with sh -x to have debug information." 1>&2; set +x; shift 1 ;;
   -R) _RETRY=0; shift 1 ;;
   -r) _RETRY="$2"; shift 2 ;;
   -t) _CONNECTION_TIMEOUT="$2"; shift 2 ;;
   -rt) _RETRY_SLEEP="$2"; shift 2 ;;
   -z) _COMPRESS_OUTPUT=true; _UNCOMPRESS=false; shift 1 ;;
   -p) _OUTPUT_PREFIX="$2"; shift 2 ;;
   -c) _CREATE_OUTPUT_DIR=true; shift 1 ;;
   -co) _CREATE_OUTPUT_DIR=true; _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -qo) _QUIET=true; _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -qco) _QUIET=true; _CREATE_OUTPUT_DIR=true; _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -d) . $2; shift 2 ;;
   -U | --no-uzip) _UNCOMPRESS=false; shift 1 ;;
   -o) _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -O) _OVERWRITE=true; _OUTPUT_DIR="$2"; shift 2 ;;
   -H) _FOLLOW_HTML=false; shift 1 ;;
   -x) _DIR_EXCLUDE_PATTERN="$2"; shift 2 ;;
   -w) _TMP_DIR="$2"; shift 2 ;;
   -h | --help) usage ;;
   -K) _SESSION_FILE=/dev/null; _AUTH_DATA_FILE=; shift 1 ;;
   -C) _USER_USERNAME="${2%%:*}"; _USER_PASSWORD="${2#*:}"; shift 2 ;;
   *) if [[ "$1" == "-" ]]; then
        _INPUT_URI="$_INPUT_URI `tr '\n' ' '`"
      elif [[ "${1:0:1}" == "-" ]]; then
        echo "[ERROR  ][secp] Unnknown option: $1" 1>&2
        exit 255
      else
        _INPUT_URI="$_INPUT_URI $1"
      fi
      shift 1
    ;;
  esac
done
_INPUT_URI="${_INPUT_URI# } "

#Trap the abort message (kill all the childs of this executable if you abort)
function killchild {
  for process in `ps -Af | gawk -vPID="$1" '{if ($3 == PID) printf "$2 ";}'`; do
    killchild $process
    kill -9 $process
  done
}

function terminate {
        echo "[WARNING][secp] termination signal received, aborting" 1>&2
        killchild $$
        echo "[ERROR  ][secp] aborted!" 1>&2
}
trap 'terminate; exit 255' HUP TERM

#Check output path
[[ "${_OUTPUT_DIR:0:1}" != "/" ]] && _OUTPUT_DIR="$PWD/$_OUTPUT_DIR"
if [ ! -d "$_OUTPUT_DIR" ]; then
  if $_CREATE_OUTPUT_DIR; then
    mkdir -p "$_OUTPUT_DIR" || { echo "[ERROR  ][secp] Failed to create output directory '$_OUTPUT_DIR'" 1>&2; exit 254; }
  else
    echo "[ERROR  ][secp] the output directory '$_OUTPUT_DIR' does not exist" 1>&2
    exit 254
  fi
fi
_OUTPUT_DIR="${_OUTPUT_DIR%/}/"

##Local functions

#loadpass function manages internal credentials storage
#Usage:
# loadpass <key> <mandatory>
#  where <key> is the index associated to the password entry in the storage
#  <mandatory>, if true, defines the password as mandatory, so it will be asked
#  to the user if it is not present in the storage
function loadpass {
  CRED_INDEX="$1"
  _SERVER_USERNAME=
  _SERVER_PASSWORD=

  #Check if credentials exists (if so, load them)
  if [[ -n "$_AUTH_DATA_FILE" && -e $_AUTH_DATA_FILE ]]; then
    if [[ -n "$_USER_USERNAME" && -n "$_USER_PASSWORD" ]]; then
      #Save credentials in the data file
      _SERVER_USERNAME="$_USER_USERNAME"
      _SERVER_PASSWORD="$_USER_PASSWORD"
      CRED_INDEX="${URI#*://}"; CRED_INDEX=${CRED_INDEX%%/*}
      sed -i "\|^$CRED_INDEX=.*|d" $_AUTH_DATA_FILE
      echo "$CRED_INDEX=$_SERVER_USERNAME:$_SERVER_PASSWORD" >> $_AUTH_DATA_FILE
    else
      CREDS="`sed -n "s|^$CRED_INDEX"'=\(.*\)$|\1|p' $_AUTH_DATA_FILE`"
      _SERVER_USERNAME="${CREDS%%:*}"
      _SERVER_PASSWORD="${CREDS#*:}"
    fi
  fi

  if [[ -z "$_SERVER_USERNAME" && -z "$_SERVER_PASSWORD" && "$2" == "true" && "$_QUIET" == "false" ]]; then
    #If credentials are mandatory, ask them to the user (if quiet mode is not specified)
    echo "[INTER  ][secp] Credentials are mandatory for $CRED_INDEX. Please insert your username:" 1>&2
    read _SERVER_USERNAME
    echo "[INTER  ][secp] And your password:" 1>&2
    read -s _SERVER_PASSWORD
    echo -n "[INTER  ][secp] Do you want to store your password? [y/n] (NOTE: username/password will be stored in clear text): " 1>&2
    read -n 1 YESNO
    echo ""
    if [[ $YESNO == "y" ]]; then
      echo "$CRED_INDEX=$_SERVER_USERNAME:$_SERVER_PASSWORD" >> $_AUTH_DATA_FILE
    fi
  fi
}


##DEFINE DRIVERS
###############################################################################
#                          Drivers                                            #
###############################################################################
# calling sequence:
# driver <url> <output-file>
#
# environment variables
#  _RETRY               maximum number of retries
#  _RETRY_SLEEP         sleep time between retries
#  _CONNECTION_TIMEOUT  connection timeout
#  _COPY                if true, the driver should copy the file. If false, the
#                       driver can make a symbolic link to the original file
#
# output status conventions:
# 0       : operation successfull
# 1       : file does not exist, may try to fecth the gzipped version
# < 128   : error during operation but worth retry
# >= 128  : fatal error during operation, code will be translated as (256 - code)
# 128     : a timeout occured
# 129     : generic error code for unhandled fatal errors (or -127)
###############################################################################

#Local file, file:// or nfs:// URIs
function fileDriver ()
{
  INPUT_URI="${1#*://}"
  [[ -e "$INPUT_URI" ]] || return 1

  if $_COPY; then
    if [ -d $INPUT_URI ]; then
      if [[ -n "$_DIR_EXCLUDE_PATTERN" ]]; then
        rsync -a --exclude "$_DIR_EXCLUDE_PATTERN" $INPUT_URI `dirname $2`
        res=$?
      else
        rsync -a $INPUT_URI `dirname $2`
        res=$?
      fi
    else
      curl -k -f --create-dirs --connect-timeout $_CONNECTION_TIMEOUT --retry $_RETRY --retry-delay $_RETRY_SLEEP -o "$2" "file://$INPUT_URI" 1>&2
      res=$?
    fi
  else
    $_OVERWRITE && ln -fs "$INPUT_URI" "$2" || ln -s "$INPUT_URI" "$2"
    res=$?
  fi

  return $res
}

#hdfs (HadoopFS Driver), needs _CIOP_SHARE_PATH to be set into the environment to set nfs mount point
function hdfsDriver ()
{
  INPUT_URI=`sed "s#//*#/#g" <<<"${1#*://}"`

  [[ -n "$_CIOP_SHARE_PATH" && -d "$_CIOP_SHARE_PATH/tmp" ]] || {
        echo "[ERROR ][ciop-copy][failed] Environement variable _CIOP_SHARE_PATH is not set. Set the HDFS mount point to _CIOP_SHARE_PATH and retry." 1>&2
        return 1
  }

  hadoop dfs -lsr $INPUT_URI | while read perm inode user group size date time path; do
    [ "$perm" == "Found" ] && continue
    [ -n "$_DIR_EXCLUDE_PATTERN" ] && [ -n "`echo $path | egrep $_DIR_EXCLUDE_PATTERN`" ] && continue
    localpath=${path#$INPUT_URI}
    [ -n "$localpath" ] && mkdir -p $2
    if [ "`echo $perm | cut -c1`" == "d" ]; then
      mkdir -p ${2}${localpath}
    else
      if $_COPY; then
        rsync -a $_CIOP_SHARE_PATH/$path ${2}${localpath}
        [ $? == 0 ] || exit 127
      else
        if [ $_OVERWRITE == false ]; then
          ln -s $_CIOP_SHARE_PATH/$path ${2}${localpath}
        else
          ln -fs $_CIOP_SHARE_PATH/$path ${2}${localpath}
        fi
      fi
    fi
  done
  res=$?

  return $res
}

#Download the files using the ssh+scp server
function scpDriver ()
{
  HOST="`echo $1 | cut -d '/' -f 3`"
  USER="`echo $HOST | cut -d ':' -f 1`"
  if [[ "$USER" == "$HOST" ]]; then
    USER="`echo $HOST | cut -d '@' -f 1`"
    [[ "$USER" == "$HOST" ]] && USER="" || HOST="`echo $HOST | cut -d '@' -f 2`"
  else
    HOST="`echo $HOST | cut -d ':' -f 2`"
    PASS="`echo $HOST | cut -d '@' -f 1`"
    [[ "$PASS" == "$HOST" ]] && PASS="" || HOST="`echo $HOST | cut -d '@' -f 2`"
  fi
  PT="/`echo $1 | cut -d '/' -f 4-`"
  [[ -z "$USER" ]] && USERHOST=$HOST || USERHOST=$USER@$HOST
  [[ -z "$_IDENTITY_FILE" ]] || USERHOST="-i $_IDENTITY_FILE $USERHOST"
  NR="$_RETRY"
  while [[ "$NR" -gt 0 ]]; do
    scp -o ConnectTimeout=$_CONNECTION_TIMEOUT -o ConnectionAttempts=$_RETRY -o StrictHostKeyChecking=no $USERHOST:$PT $_LOCAL_FILE 2>&1 | gawk 'BEGIN{res=0;}{if ($0 ~ "No such file") res=1;}END{exit res}'
    res=${PIPESTATUS[0]}
    [[ "$res" -eq "0" ]] && break
    [[ "${PIPESTATUS[1]}" -eq "1" ]] && break # do not retry if there is no file
    [[ "$res" -eq "1" ]] && res=129 # error code gives no information on the type of the error
    NR=$(( $NR - 1 ))
    sleep $_RETRY_SLEEP
  done

  return $res
}

#driver for the GridFTP protocol (gridftp:// and gsiftp:// URIs)
function gridftpDriver()
{
  #Download with globus-url-copy (timeout and retries are managed by the driver, since globus-url-copy do not manage them)
  NR=$_RETRY
  while [[ "$NR" -gt 0 ]]; do
    NF=$_CONNECTION_TIMEOUT
    globus-url-copy -dbg -b -r "gsiftp://${1#*://}" "file://$2" 2>&1 | gawk 'BEGIN{res=127;}{ if ($0 ~ "debug: operation complete$") res=0; if ($0 ~ "^error:") res=127; if ($0 ~ "No such file or directory") {res=1; exit; }; if ($0 ~ "Error with gss credential handle") {res=2; exit; }; }END{exit res}' &
    iii=$!
    while [[ -d "/proc/$iii" && "$NF" -gt "0" ]]; do
      NF=$(( $NF - 1 ))
      sleep 1
    done
    [ "$NF" -eq "0" ] && kill -9 $iii
    wait $iii
    res=$?
    [[ "$res" -eq "0" ]] && break
    if [[ "$res" -eq "1" ]]; then
      echo "[ERROR  ][secp][failed] url '$URI' not found" 1>&2
      break
    fi
    if [[ "$res" -eq "2" ]]; then
      echo "[ERROR  ][secp][failed] not authorized to access url '$URI'" 1>&2
      break
    fi
    NR=$(( $NR - 1 ))
    sleep $_RETRY_SLEEP
  done

  [[ "$res" -ne "0" ]] && rm -f $2
  return $res
}

#cahce 1.0 protocol cache://
function cacheDriver()
{
  NR="$_RETRY"
  while [[ "$NR" -gt 0 ]]; do
    grid-cache-client "$1" "file://$_LOCAL_FILE" 2>&1 | gawk 'BEGIN{res=0;}{if ($0 ~ "No such file") res=1;}END{exit res}'
    res=${PIPESTATUS[0]}
    [[ "$res" -eq "0" ]] && break
    [[ "${PIPESTATUS[1]}" -eq "1" ]] && break # do not retry if there is no file
    [[ "$res" -eq "1" ]] && res=129 # error code gives no information on the type of the error
    NR=$(( $NR - 1 ))
    sleep $_RETRY_SLEEP
  done

  return $res
}

#Generic driver for curl, works for http, ftp and any other URL supported cURL
function curlDriver {
  curlopt="-b $_SESSION_FILE -c $_SESSION_FILE -L -f --connect-timeout $_CONNECTION_TIMEOUT --retry $_RETRY --retry-delay $_RETRY_SLEEP"
  if [[ -n "$_SERVER_PASSWORD" && -n "$_SERVER_USERNAME" ]]; then
    curlopt="$curlopt -k --user $_SERVER_USERNAME:$_SERVER_PASSWORD"
  elif [[ -n "$X509_USER_PROXY" ]]; then
    curlopt="$curlopt --cert $X509_USER_PROXY --key $X509_USER_PROXY --cacert $X509_USER_PROXY"
    [[ -z "$X509_CERT_DIR" ]] && X509_CERT_DIR=/etc/grid-security/certificates/
    [[ -d "$X509_CERT_DIR" ]] && curlopt="$curlopt --capath $X509_CERT_DIR"
  else
    curlopt="$curlopt -k"
  fi

  #Try to download the file
  message="`curl $curlopt -o "$2" "$1" 2>&1`"
  res=$?
  if [[ "$res" -ne "0" ]]; then
    if [[ "${message/The requested URL returned error: 403//}" != "$message" ]]; then
      echo "[ERROR  ][secp] Forbidden. Please check your proxy certificate or your username/password." 1>&2
      res=2
    elif [[ "${message/The requested URL returned error: 404//}" != "$message" ]]; then
      res=1
    else
      echo "[ERROR  ][secp][failed] url '$URI' - $message" 1>&2
    fi
    rm -f "$2"
    return $res
  fi

  return 0
}

function s3Driver ()
{
  s3cmd get "$1" "$2" | gawk 'BEGIN{res=0;}{if ($0 ~ "[nN]o such file") res=1; if ($0 ~ "[nN]ame or service not known") res=2; if ($0 ~ "[Hh]ost key verification failed") res=3; if ($0 ~ "[pP]ermission denied .*(publickey|password)") res=129;}END{exit res}'
  return ${PIPESTATUS[1]}
}

function httpDriver { curlDriver $@; }
function ftpDriver { curlDriver $@; }
function httpsDriver { curlDriver $@; }
function nfsDriver { fileDriver $@; }
function gsiftpDriver { gridftpDriver $@; }

##END DRIVERS DEFINITION

#Download the files one by one
exit_code=0
if $_ABORT_ON_URI_ERROR; then
  ON_ERROR='exit $res'
else
  ON_ERROR='exit_code=1; continue'
fi
while [[ -n "$_INPUT_URI" ]]; do
  URI="${_INPUT_URI%% *}"
  _INPUT_URI="${_INPUT_URI#* }"
  [[ -z "$URI" ]] && continue

  echo "[INFO   ][secp][starting] url '$URI' > local '$_OUTPUT_DIR$_OUTPUT_PREFIX'" 1>&2

  #Search if local file already exists (if so, overwrite or exit)
  _LOCAL_FILE="${URI##*/}"; _LOCAL_FILE="${_LOCAL_FILE%\?*}"; _LOCAL_FILE="$_OUTPUT_DIR$_OUTPUT_PREFIX$_LOCAL_FILE"
  if [[ -e "$_LOCAL_FILE" ]]; then
    if $_OVERWRITE; then
      echo "[WARNING][secp][get:file] clearing existing sink '$_LOCAL_FILE'" 1>&2
      rm -rf $_LOCAL_FILE 
    elif $_SKIP_EXISTING_SINKS; then
      echo "[WARNING][secp][get:file] skipping existing sink '$_LOCAL_FILE'" 1>&2
      echo "[INFO   ][secp][success] url '$URI' > local '$_LOCAL_FILE'" 1>&2
      $_QUIET ||  echo $_LOCAL_FILE
      continue
    else
      echo "[ERROR  ][secp][failed] sink '$_LOCAL_FILE' already exists" 1>&2
      res=251
      eval "$ON_ERROR"
    fi
  fi

  #Load credentials stored in the home for the user (if present). Credentials are indexed using the server name and port.
  CRED_INDEX="${URI#*://}"; CRED_INDEX=${CRED_INDEX%%/*}
  loadpass $CRED_INDEX

  #Map URI to driver. Simple mapping is done using URI_PROTOCOL, but more complex mapping can be inplemented (ex. in case of cache protocol, etc...).
  #Drivers shall satisfy the options in the comments above
  [[ "$URI" =~ [a-zA-Z0-9]*:\/\/ ]] || URI="file://$URI"
  URI_DRIVER="${URI%%://*}Driver"
  
  #Check if driver exists
  if [[ "`type -t $URI_DRIVER`" != "function" ]]; then
	echo "[ERROR  ][secp][failed] protocol ${URI%%://*}:// not supported" 1>&2
	rm -f $_LOCAL_FILE
	res=1
	eval "$ON_ERROR"    
  fi

  #Call the driver
  $URI_DRIVER $URI $_LOCAL_FILE
  res=$?
  if [[ "$res" -eq "1" ]]; then
    if [[ "${URI##*.}" != "gz" ]]; then
      #the file do not exist, try with the .gz extension
      _INPUT_URI="${URI}.gz $_INPUT_URI"
      continue
    else
      echo "[ERROR  ][secp][failed] url '$URI' not found" 1>&2
      res=251
      eval "$ON_ERROR"
    fi
  fi
  [[ "$res" -ne "0" ]] && eval "$ON_ERROR"

  # Follow HTML (perform also HTML authentication)
  if $_FOLLOW_HTML; then
    if [[ ! -d "$_LOCAL_FILE" && ! -h "$_LOCAL_FILE" && "$(stat -c%s "$_LOCAL_FILE")" -lt 10240 ]]; then
      URI_TO_FOLLOW=
      #check file format by file URI name/path and the first 5 characters
      FB="`head -c 5 $_LOCAL_FILE`"
      if [[ "${URI##*.}" == "uar" ]]; then
        #This is an UAR archive
        URI_TO_FOLLOW="`cat $_LOCAL_FILE`"
      elif [[ "${URI##*/}" =~ ^rdf && "$FB" == "<?xml" ]]; then
        #It seems to be an XML. Try to extract the RDF tags (and do not duplicate the entries)
        URI_TO_FOLLOW=`tr -d '\n' < $_LOCAL_FILE | gawk 'BEGIN{RS=">";p=0}{ if ($1 ~ "<ws:[A-Z]*") { split($2,a,"\""); if (a[1] ~ "^rdf:about") { b=a[2]; sub(/^.*\//,"",b); sub(/\?.*$/,"",b); if (d[b]==0) { d[b]=1; e[p]=a[2]; p++; } } } }END{ for (i=0;i<p;i++) printf e[i]" "; exit p;}'`
      elif [[ "$FB" == "<!DOC" || "$FB" == "<!doc"  || "$FB" == "<html" || "$FB" == "<HTML"  ]]; then
        #It seems to be an HTML. Try to extract the RDF tags (and do not duplicate the entries)
        URI_TO_FOLLOW=`tr -d '\n' < $_LOCAL_FILE | gawk 'BEGIN{RS=">";p=0}{ if ($1" "$2 == "<meta http-equiv=\"refresh\"") { split($3,a,"\""); e[0]="refresh://"a[2]; p=1; exit; } ; if ($1 == "<a") { split($2,a,"\""); if (a[1] == "href=") { b=a[2]; sub(/^.*\//,"",b); sub(/\?.*$/,"",b); if (d[b]==0) { d[b]=1; e[p]=a[2]; p++; } } } }END{ for (i=0;i<p;i++) printf e[i]" "; exit p;}'`
        if [[ "$URI_TO_FOLLOW" =~ ^refresh:// ]]; then
          #this is a meta-refresh
          URI_TO_FOLLOW=${URI_TO_FOLLOW#refresh://}
          ST=${URI_TO_FOLLOW%%;*}
          if [[ "$ST" == "$URI_TO_FOLLOW" ]]; then
            URI_TO_FOLLOW="$URI "
          else
            URI_TO_FOLLOW="${URI_TO_FOLLOW#*;}"
            URI_TO_FOLLOW="${URI_TO_FOLLOW# }"
          fi
          echo "[INFO   ][secp][refresh] got meta-refresh, waiting for $ST seconds" 1>&2
          sleep $ST
        fi
      elif grep -q '<title>EO SSO</title>' $_LOCAL_FILE; then
        #This seems to be the EO-SSO login page, perform EO-SSO login
        echo "[INFO   ][secp][auth] EO-SSO login page detected. Logging in and re-downloading the file..." 1>&2

        #Check if the page version and other pre-requisites
        IDP_ADDR="`sed -n -e 's|^[^<]*<a href="\(.*\)/idp/umsso20/admin">Forgot your password?</a>.*$|\1/idp/umsso20/login?null|p' $_LOCAL_FILE`"
        if [[ $? -ne 0 || -z "$IDP_ADDR" ]] || ! grep -q untilbrowserclose $_LOCAL_FILE || ! grep -q oneday $_LOCAL_FILE; then
          echo "[ERROR  ][secp][auth] EO-SSO page invalid. Maybe EO-SSO has been upgraded."
          eval "$ON_ERROR"
        fi
        if [[ $_SESSION_FILE == "/dev/null" || ! -w $_SESSION_FILE ]]; then
          echo "[ERROR  ][secp][auth] You need to enable session file to perform EO-SSO login."
          eval "$ON_ERROR"
        fi

        #Load UM-SSO password (this is mandatory)
        SSO_SERVER=${IDP_ADDR#*://}; SSO_SERVER=${SSO_SERVER%%:*};
        loadpass "$SSO_SERVER" true
    
        #Try to re-download the file (performing log in...)
        curl -b $_SESSION_FILE -c $_SESSION_FILE -L -k -f -s -S "$IDP_ADDR" --data "cn=$_SERVER_USERNAME&password=$_SERVER_PASSWORD"'&loginFields=cn@password&loginMethod=umsso&sessionTime=untilbrowserclose&idleTime=oneday' -o $_LOCAL_FILE
        res=$?

        #Check if you re-get the EO-SSO page
        if [[ "$res" -ne "0" ]] || [[ "$(stat -c%s "$_LOCAL_FILE")" -lt 10240 && $(grep -c '<title>EO SSO</title>' $_LOCAL_FILE) -eq 1 ]]; then
          echo "[ERROR  ][secp][auth] Failed to login into EO-SSO. Please check your authentication credentials."
          eval "$ON_ERROR"
        fi
      fi
      #Add the URI_TO_FOLLOW to the URI list
      if [[ -n "$URI_TO_FOLLOW" ]]; then
        echo "[INFO   ][secp][success] got URIs '$URI_TO_FOLLOW'" 1>&2
        _INPUT_URI="$URI_TO_FOLLOW$_INPUT_URI"
        rm -f $_LOCAL_FILE
        continue
      fi
    fi
  fi

  # Uncompress the file
  if $_UNCOMPRESS; then
    case ${_LOCAL_FILE##*.} in
      gz)
        echo "[INFO   ][secp][unpack:gz] got url as '${_LOCAL_FILE##*/}' - unpacking" 1>&2
        _LOCAL_FILE=${_LOCAL_FILE%.gz}
        gunzip -c ${_LOCAL_FILE}.gz > ${_LOCAL_FILE}
        res=$?
        if [[ "$res" -ne "0" ]]; then
          echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE' failed (gunzip returned $res)" 1>&2
          eval "$ON_ERROR"
        fi
        rm -f ${_LOCAL_FILE}.gz
      ;;
      tgz)
        echo "[INFO   ][secp][unpack:tgz] got url as '${_LOCAL_FILE##*/}' - unpacking" 1>&2
        _LOCAL_FILE="${_LOCAL_FILE%.tgz}"
        tar xz -C $_OUTPUT_DIR -f $_LOCAL_FILE.tgz
        res=$?
        if [[ "$res" -ne "0" ]]; then
          echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE' failed (tar returned $res)" 1>&2
          eval "$ON_ERROR"
        fi
        rm -f $_LOCAL_FILE.tgz
      ;;
      zip)
        echo "[INFO   ][secp][unpack:zip] got url as '${_LOCAL_FILE##*/}' - unpacking" 1>&2
        _LOCAL_FILE=${_LOCAL_FILE%.zip}
        mkdir $_LOCAL_FILE
        unzip -qq -o $_LOCAL_FILE.zip -d $_LOCAL_FILE
        res=$?
        if [[ "$res" -ne "0" ]]; then
          rmdir $_LOCAL_FILE
          echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE.zip' failed (unzip returned $res)" 1>&2
          eval "$ON_ERROR"
        fi
        rm -f $_LOCAL_FILE.zip
        case `find $_LOCAL_FILE -mindepth 1 -maxdepth 1 | wc -l` in
          0) echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE.zip' failed (empty archive)" 1>&2
             eval "$ON_ERROR"
          ;;
          1) EXTRACTED_LFILE="`find $_LOCAL_FILE -mindepth 1 -maxdepth 1`"
             mv $EXTRACTED_LFILE ${_LOCAL_FILE%/*}/tmpfilename$$
             rmdir $_LOCAL_FILE &>/dev/null
             mv ${_LOCAL_FILE%/*}/tmpfilename$$ "${_LOCAL_FILE%/*}/${EXTRACTED_LFILE##*/}"
             _LOCAL_FILE="${_LOCAL_FILE%/*}/${EXTRACTED_LFILE##*/}"
          ;;
        esac
      ;;
     *)
       ;;
    esac
  fi

  #Compress the file
  if $_COMPRESS_OUTPUT; then
    if [[ -h "$_LOCAL_FILE" ]]; then
      FILE_TO_PACK="`readlink -f $_LOCAL_FILE`"
    else
      FILE_TO_PACK="$_LOCAL_FILE"
    fi
    if [[ -d "$FILE_TO_PACK" ]]; then
      FILE_TO_PACK="${FILE_TO_PACK%/}"
      echo "[INFO   ][secp][pack:tgz] got url as '${_LOCAL_FILE##*/}' - packing" 1>&2
      tar cz -C ${FILE_TO_PACK%/*} -f $_LOCAL_FILE.tgz ${FILE_TO_PACK##*/}
      res=$?
      if [[ "$res" -ne "0" ]]; then
        echo "[ERROR  ][secp][failed] paking '$_LOCAL_FILE' failed (tar returned $res)" 1>&2
        eval "$ON_ERROR"
      fi
      rm -rf $_LOCAL_FILE
      _LOCAL_FILE="$_LOCAL_FILE.tgz"
    else
      if [[ "${FILE_TO_PACK##*.}" == "gz" || "${FILE_TO_PACK##*.}" == "tgz" || "${FILE_TO_PACK##*.}" == "zip" ]]; then
        echo "[INFO   ][secp][pack:${_LOCAL_FILE##*.}] got url as '${_LOCAL_FILE##*/}' - already packed" 1>&2
      else
        echo "[INFO   ][secp][pack:gz] got url as '${_LOCAL_FILE##*/}' - packing" 1>&2
        gzip -c $FILE_TO_PACK > $_LOCAL_FILE.gz
        res=$?
        if [[ "$res" -ne "0" ]]; then
          echo "[ERROR  ][secp][failed] paking '$_LOCAL_FILE' failed (gzip returned $res)" 1>&2
          eval "$ON_ERROR"
        fi
        rm -f $_LOCAL_FILE
        _LOCAL_FILE="$_LOCAL_FILE.gz"
        [[ ${myfile/#*zip/ZIP} == "ZIP" ]] && secp_flags=""
      fi
    fi
  fi

  echo "[INFO   ][secp][success] url '$URI' > local '$_LOCAL_FILE'" 1>&2

  # Echo the file to stdout
  $_QUIET || echo $_LOCAL_FILE
done

exit $exit_code
