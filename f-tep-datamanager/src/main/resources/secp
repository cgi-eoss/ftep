#!/bin/env bash
# Project: secp
SECP_VERSION=3.1.3

# Usage:
function usage {
cat <<:usage
secp version $SECP_VERSION
Usage:
secp  [-h] [-a] [-q] [-f] [-b <url-base>] [-d <driver-def>]
      [-o|O <sink-dir>] [-w <work-dir>] [-c] [-p <prefix>] [-z|-Z]
      [-r <num-retries>] [-t <timeout>] [-R] [-l <log-label>] [-D]
      <url1> [<url2> ... <urlN>]

URL Parameters:
      Arguments are URL strings provided by seurl
      if a parameter is specified as '-', URLs are read and inserted from standard input

Options:
      -h               displays this help page
      -a               abort on first error without attempting to process further URLs
      -q               quiet mode, local filenames are not echoed to stdout after transfer
      -f               force transfer of a physical copy of the file in case of nfs URLs
      -F <url-list>    get URLs from the <url-list> file
      -b <url-base>    ????
      -d <driver-file> get additional drivers from shell file <driver-file>. Drivers shall contain
                       a named <protocol>Driver
      -o|O <out-dir>   defines the output directory for transfers (default is $PWD)
                       with -O the sink files or directories possibly existing in the output
                       directory will be overwritten.
      -c               creates the output directory if it does not exist
      -p <prefix>      prepend the given prefix to all output names
      -z               provide output as a compressed package (.gz for files or .tgz for folders). NOTE
                       that it will not compress already compressed files (.gz, .tgz or .zip)
      -Z               ????
      -U|--no-uzip     disable file automatic decompression of .gz, .tgz and .zip files.
      -r <num-retries> defines the maximum number of retries (default is 5)
      -rt <seconds>    define the time (in seconds) between retries (default is 60)
      -t <timeout>     defines the timer (in seconds) for the watchdog timeout applicable to
                       gridftp, scp, ftp, http, and https schemes (default is 600 seconds)
      -R               do not retry transfer after timeout
      -l <log-label>   ????
      -D               set debug mode for command output parsing debugging
      -H               do not follow html and rdf tags and .uar archives.
      -s               skip download if sink path already exists
      -w <tmpdir>      set up temporary directory for drivers (default to /tmp)
      -x <pattern>     exclude the files matching the pattern for directory input
      -K               private mode, disable storing of authentication session in ~/.secp_sess file
                       and passwords in the ~/.secp_cred file.
      -C <user>:<pass> force <user> and <pass> authentication (NOTE: these passwords will be stored
                       in clear text in the ~/.secp_cred file. NOTE: If username/passowrd is specified
                       it has precedence over the certificate/proxy authentication)

Output:
      unless the quiet option is used (-q), the local path of each file (or directory)
      downloaded after each URL transfer is echoed, one per line
      unless the -U option is used, if the output file is a .gz or .tgz file it will be
      decompressed
      unless the -H options is specified, the software will follow the RDF <dclite4g:onlineResource>
      and the HTML href and refresh tags.
      the software will perform authentication if credentials are specified. Supported authentication
      types are 'basic', EO-SSO and session cookies. Credentials are stored in the ~/.secp_cred file,
      session cookies are stored in the ~/.secp_sess file (use -K switch to disable this behaviour)

Exit codes:
      0      all URLs were successfully downloaded
      1      an error occured during processing
      255    environment is invalid (e.g. invalid working directory) or invalid options are provided
      254    output directory does not exist or failed creating it (with -c option)

      if the -a option is used, the exit code is set to the error code of the last URL transfer:
      252    no driver available for URL
      251    an existing file or directory conflicts with the sink for the URL in the output directory
      250    an error occured while unpacking the output file or when packaging/compressing the output
             file (when -z or -Z option is used)
      128    a timeout occured while fetching an url
      127    a fatal error occured, source of error is not known or not handled by driver
      <128   error codes specific to the transfer scheme
      1      resource pointed by input URL do not exist
:usage
  exit 0
}

[[ -z "$1" ]] && usage

#Default parameters
_SKIP_URI_ERRORS=true
_ABORT_ON_URI_ERROR=false
_QUIET=false
_COPY=false
_RETRY=5
_RETRY_SLEEP=60
_CONNECTION_TIMEOUT=600
_COMPRESS_OUTPUT=false
_OUTPUT_PREFIX=
_CREATE_OUTPUT_DIR=false
_OUTPUT_DIR="$PWD"
_INPUT_URI=
_UNCOMPRESS=true
_FOLLOW_HTML=true
_OVERWRITE=false
_SKIP_EXISTING_SINKS=false
_TMP_DIR=/tmp
_DIR_EXCLUDE_PATTERN=
_USER_USERNAME=
_USER_PASSWORD=
_AUTH_DATA_FILE="/home/ftep/.secp_cred"
_SESSION_FILE="/home/ftep/.secp_sess"

cd /data/cache/

#Parse arguments
while [[ "$#" -gt 0 ]]; do
  case "$1" in
   -a) _ABORT_ON_URI_ERROR=true; shift 1 ;;
   -s) _SKIP_EXISTING_SINKS=true; shift 1 ;;
   -q) _QUIET=true; shift 1 ;;
   -f) _COPY=true; shift 1 ;;
   -F) [[ -e "$2" ]] && _INPUT_URI="$_INPUT_URI `tr '\n' ' '<$2`"; shift 2 ;;
   -D) echo "[WARNING] -D option is deprecated. Launch this script with sh -x to have debug information."; set +x; shift 1 ;;
   -R) _RETRY=0; shift 1 ;;
   -r) _RETRY="$2"; shift 2 ;;
   -t) _CONNECTION_TIMEOUT="$2"; shift 2 ;;
   -rt) _RETRY_SLEEP="$2"; shift 2 ;;
   -z) _COMPRESS_OUTPUT=true; _UNCOMPRESS=false; shift 1 ;;
   -p) _OUTPUT_PREFIX="$2"; shift 2 ;;
   -c) _CREATE_OUTPUT_DIR=true; shift 1 ;;
   -co) _CREATE_OUTPUT_DIR=true; _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -qo) _QUIET=true; _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -qco) _QUIET=true; _CREATE_OUTPUT_DIR=true; _OVERWRITE=false; _OUTPUT_DIR="$2"; shift 2 ;;
   -d) . $2; shift 2 ;;
   -U | --no-uzip) _UNCOMPRESS=false; shift 1 ;;
   -o) _OVERWRITE=true; _OUTPUT_DIR="$2"; shift 2 ;;
   -O) _OVERWRITE=true; _OUTPUT_DIR="$2"; shift 2 ;;
   -H) _FOLLOW_HTML=false; shift 1 ;;
   -x) _DIR_EXCLUDE_PATTERN="$2"; shift 2 ;;
   -w) _TMP_DIR="$2"; shift 2 ;;
   -h | --help) usage ;;
   -K) _SESSION_FILE=/dev/null; _AUTH_DATA_FILE=; shift 1 ;;
   -C) _USER_USERNAME="${2%%:*}"; _USER_PASSWORD="${2#*:}"; shift 2 ;;
   *) if [[ "$1" == "-" ]]; then
        _INPUT_URI="$_INPUT_URI `tr '\n' ' '`"
      elif [[ "${1:0:1}" == "-" ]]; then
        echo "[ERROR  ][secp] Unnknown option: $1"
        exit 255
      else
        _INPUT_URI="$_INPUT_URI $1"
      fi
      shift 1
    ;;
  esac
done
_INPUT_URI="${_INPUT_URI# } "

#Trap the abort message (kill all the childs of this executable if you abort)
function killchild {
  for process in `ps -Af | gawk -vPID="$1" '{if ($3 == PID) printf "$2 ";}'`; do
    killchild $process
    kill -9 $process
  done
}

function terminate {
        echo "[WARNING][secp] termination signal received, aborting"
        killchild $$
        echo "[ERROR  ][secp] aborted!"
}
trap 'terminate; exit 255' HUP TERM

#Check output path
[[ "${_OUTPUT_DIR:0:1}" != "/" ]] && _OUTPUT_DIR="$PWD/$_OUTPUT_DIR"
if [ ! -d "$_OUTPUT_DIR" ]; then
  if $_CREATE_OUTPUT_DIR; then
    mkdir -p "$_OUTPUT_DIR" || { echo "[ERROR  ][secp] Failed to create output directory '$_OUTPUT_DIR'"; exit 254; }
  else
    echo "[ERROR  ][secp] the output directory '$_OUTPUT_DIR' does not exist"
    exit 254
  fi
fi
_OUTPUT_DIR="${_OUTPUT_DIR%/}/"

##Local functions

#loadpass function manages internal credentials storage
#Usage:
# loadpass <key> <mandatory>
#  where <key> is the index associated to the password entry in the storage
#  <mandatory>, if true, defines the password as mandatory, so it will be asked
#  to the user if it is not present in the storage
function loadpass {
  CRED_INDEX="$1"
  _SERVER_USERNAME=
  _SERVER_PASSWORD=

  #Check if credentials exists (if so, load them)
  if [[ -n "$_AUTH_DATA_FILE" && -e $_AUTH_DATA_FILE ]]; then
    if [[ -n "$_USER_USERNAME" && -n "$_USER_PASSWORD" ]]; then
      #Save credentials in the data file
      _SERVER_USERNAME="$_USER_USERNAME"
      _SERVER_PASSWORD="$_USER_PASSWORD"
      CRED_INDEX="${URI#*://}"; CRED_INDEX=${CRED_INDEX%%/*}
      sed -i "\|^$CRED_INDEX=.*|d" $_AUTH_DATA_FILE
      echo "$CRED_INDEX=$_SERVER_USERNAME:$_SERVER_PASSWORD" >> $_AUTH_DATA_FILE
    else
      CREDS="`sed -n "s|^$CRED_INDEX"'=\(.*\)$|\1|p' $_AUTH_DATA_FILE`"
      _SERVER_USERNAME="${CREDS%%:*}"
      _SERVER_PASSWORD="${CREDS#*:}"
    fi
  fi

  if [[ -z "$_SERVER_USERNAME" && -z "$_SERVER_PASSWORD" && "$2" == "true" && "$_QUIET" == "false" ]]; then
    #If credentials are mandatory, ask them to the user (if quiet mode is not specified)
    echo "[INTER  ][secp] Credentials are mandatory for $CRED_INDEX. Please insert your username:"
    read _SERVER_USERNAME
    echo "[INTER  ][secp] And your password:"
    read -s _SERVER_PASSWORD
    echo -n "[INTER  ][secp] Do you want to store your password? [y/n] (NOTE: username/password will be stored in clear text): "
    read -n 1 YESNO
    echo ""
    if [[ $YESNO == "y" ]]; then
      echo "$CRED_INDEX=$_SERVER_USERNAME:$_SERVER_PASSWORD" >> $_AUTH_DATA_FILE
    fi
  fi
}


##DEFINE DRIVERS
###############################################################################
#                          Drivers                                            #
###############################################################################
# calling sequence:
# driver <url> <output-file>
#
# environment variables
#  _RETRY               maximum number of retries
#  _RETRY_SLEEP         sleep time between retries
#  _CONNECTION_TIMEOUT  connection timeout
#  _COPY                if true, the driver should copy the file. If false, the
#                       driver can make a symbolic link to the original file
#
# output status conventions:
# 0       : operation successfull
# 1       : file does not exist, may try to fecth the gzipped version
# < 128   : error during operation but worth retry
# >= 128  : fatal error during operation, code will be translated as (256 - code)
# 128     : a timeout occured
# 129     : generic error code for unhandled fatal errors (or -127)
###############################################################################

#Local file, file:// or nfs:// URIs
function fileDriver ()
{
  INPUT_URI="${1#*://}"
  [[ -e "$INPUT_URI" ]] || return 1

  if $_COPY; then
    if [ -d $INPUT_URI ]; then
      if [[ -n "$_DIR_EXCLUDE_PATTERN" ]]; then
        rsync -a --exclude "$_DIR_EXCLUDE_PATTERN" $INPUT_URI `dirname $2`
        res=$?
      else
        rsync -a $INPUT_URI `dirname $2`
        res=$?
      fi
    else
      curl --remote-name --keepalive-time 2 -k -f --create-dirs --connect-timeout $_CONNECTION_TIMEOUT --retry $_RETRY --retry-delay $_RETRY_SLEEP "file://$INPUT_URI"
      res=$?
    fi
  else
    $_OVERWRITE && ln -fs "$INPUT_URI" "$2" || ln -s "$INPUT_URI" "$2"
    res=$?
  fi

  return $res
}

#hdfs (HadoopFS Driver), needs _CIOP_SHARE_PATH to be set into the environment to set nfs mount point
function hdfsDriver ()
{
  INPUT_URI=`sed "s#//*#/#g" <<<"${1#*://}"`

  [[ -n "$_CIOP_SHARE_PATH" && -d "$_CIOP_SHARE_PATH/tmp" ]] || {
        echo "[ERROR ][ciop-copy][failed] Environement variable _CIOP_SHARE_PATH is not set. Set the HDFS mount point to _CIOP_SHARE_PATH and retry."
        return 1
  }

  hadoop dfs -lsr $INPUT_URI | while read perm inode user group size date time path; do
    [ "$perm" == "Found" ] && continue
    [ -n "$_DIR_EXCLUDE_PATTERN" ] && [ -n "`echo $path | egrep $_DIR_EXCLUDE_PATTERN`" ] && continue
    localpath=${path#$INPUT_URI}
    [ -n "$localpath" ] && mkdir -p $2
    if [ "`echo $perm | cut -c1`" == "d" ]; then
      mkdir -p ${2}${localpath}
    else
      if $_COPY; then
        rsync -a $_CIOP_SHARE_PATH/$path ${2}${localpath}
        [ $? == 0 ] || exit 127
      else
        if [ $_OVERWRITE == false ]; then
          ln -s $_CIOP_SHARE_PATH/$path ${2}${localpath}
        else
          ln -fs $_CIOP_SHARE_PATH/$path ${2}${localpath}
        fi
      fi
    fi
  done
  res=$?

  return $res
}

#Download the files using the ssh+scp server
function scpDriver ()
{
  HOST="`echo $1 | cut -d '/' -f 3`"
  USER="`echo $HOST | cut -d ':' -f 1`"
  if [[ "$USER" == "$HOST" ]]; then
    USER="`echo $HOST | cut -d '@' -f 1`"
    [[ "$USER" == "$HOST" ]] && USER="" || HOST="`echo $HOST | cut -d '@' -f 2`"
  else
    HOST="`echo $HOST | cut -d ':' -f 2`"
    PASS="`echo $HOST | cut -d '@' -f 1`"
    [[ "$PASS" == "$HOST" ]] && PASS="" || HOST="`echo $HOST | cut -d '@' -f 2`"
  fi
  PT="/`echo $1 | cut -d '/' -f 4-`"
  [[ -z "$USER" ]] && USERHOST=$HOST || USERHOST=$USER@$HOST
  [[ -z "$_IDENTITY_FILE" ]] || USERHOST="-i $_IDENTITY_FILE $USERHOST"
  NR="$_RETRY"
  while [[ "$NR" -gt 0 ]]; do
    scp -o ConnectTimeout=$_CONNECTION_TIMEOUT -o ConnectionAttempts=$_RETRY -o StrictHostKeyChecking=no $USERHOST:$PT $_LOCAL_FILE 2>&1 | gawk 'BEGIN{res=0;}{if ($0 ~ "No such file") res=1;}END{exit res}'
    res=${PIPESTATUS[0]}
    [[ "$res" -eq "0" ]] && break
    [[ "${PIPESTATUS[1]}" -eq "1" ]] && break # do not retry if there is no file
    [[ "$res" -eq "1" ]] && res=129 # error code gives no information on the type of the error
    NR=$(( $NR - 1 ))
    sleep $_RETRY_SLEEP
  done

  return $res
}

#driver for the GridFTP protocol (gridftp:// and gsiftp:// URIs)
function gridftpDriver()
{
  #Download with globus-url-copy (timeout and retries are managed by the driver, since globus-url-copy do not manage them)
  NR=$_RETRY
  while [[ "$NR" -gt 0 ]]; do
    NF=$_CONNECTION_TIMEOUT
    globus-url-copy -dbg -b -r "gsiftp://${1#*://}" "file://$2" 2>&1 | gawk 'BEGIN{res=127;}{ if ($0 ~ "debug: operation complete$") res=0; if ($0 ~ "^error:") res=127; if ($0 ~ "No such file or directory") {res=1; exit; }; if ($0 ~ "Error with gss credential handle") {res=2; exit; }; }END{exit res}' &
    iii=$!
    while [[ -d "/proc/$iii" && "$NF" -gt "0" ]]; do
      NF=$(( $NF - 1 ))
      sleep 1
    done
    [ "$NF" -eq "0" ] && kill -9 $iii
    wait $iii
    res=$?
    [[ "$res" -eq "0" ]] && break
    if [[ "$res" -eq "1" ]]; then
      echo "[ERROR  ][secp][failed] url '$URI' not found"
      break
    fi
    if [[ "$res" -eq "2" ]]; then
      echo "[ERROR  ][secp][failed] not authorized to access url '$URI'"
      break
    fi
    NR=$(( $NR - 1 ))
    sleep $_RETRY_SLEEP
  done

  [[ "$res" -ne "0" ]] && rm -f $2
  return $res
}

#cahce 1.0 protocol cache://
function cacheDriver()
{
  NR="$_RETRY"
  while [[ "$NR" -gt 0 ]]; do
    grid-cache-client "$1" "file://$_LOCAL_FILE" 2>&1 | gawk 'BEGIN{res=0;}{if ($0 ~ "No such file") res=1;}END{exit res}'
    res=${PIPESTATUS[0]}
    [[ "$res" -eq "0" ]] && break
    [[ "${PIPESTATUS[1]}" -eq "1" ]] && break # do not retry if there is no file
    [[ "$res" -eq "1" ]] && res=129 # error code gives no information on the type of the error
    NR=$(( $NR - 1 ))
    sleep $_RETRY_SLEEP
  done

  return $res
}

CURL_MESSAGE=''
#Generic driver for curl, works for http, ftp and any other URL supported cURL
function curlDriver {
  curlopt="-b $_SESSION_FILE -c $_SESSION_FILE -L -f --connect-timeout $_CONNECTION_TIMEOUT --retry $_RETRY --retry-delay $_RETRY_SLEEP"
  if [[ -n "$_SERVER_PASSWORD" && -n "$_SERVER_USERNAME" ]]; then
    curlopt="$curlopt -k -u $_SERVER_USERNAME:$_SERVER_PASSWORD"
  elif [[ -n "$X509_USER_PROXY" ]]; then
    curlopt="$curlopt --cert $X509_USER_PROXY --key $X509_USER_PROXY --cacert $X509_USER_PROXY"
    [[ -z "$X509_CERT_DIR" ]] && X509_CERT_DIR=/etc/grid-security/certificates/
    [[ -d "$X509_CERT_DIR" ]] && curlopt="$curlopt --capath $X509_CERT_DIR"
  else
    curlopt="$curlopt -k"
  fi
  #Try to download the file
  if [[ -n "$_USER_USERNAME" && -n "$_USER_PASSWORD" ]]; then
    userpass="-u $_USER_USERNAME:$_USER_PASSWORD"
  fi
  CURL_MESSAGE="`curl $curlopt --keepalive-time 2 $userpass --remote-name --keepalive-time 2 "$1" 2>&1`"
  res=$?
  if [[ "$res" -ne "0" ]]; then
    if [[ "${CURL_MESSAGE/The requested URL returned error: 403//}" != "$CURL_MESSAGE" ]]; then
      echo "[ERROR  ][secp] Forbidden. Please check your proxy certificate or your username/password."
      res=2
    elif [[ "${CURL_MESSAGE/The requested URL returned error: 404//}" != "$CURL_MESSAGE" ]]; then
      res=1
    else
      echo "[ERROR  ][secp][failed] url '$URI' - $CURL_MESSAGE"
    fi
    rm -f "$2"
    return $res
  else
    echo downloaded: $CURL_MESSAGE
  fi
  return 0
}

function s3Driver ()
{
  s3cmd get "$1" "$2" | gawk 'BEGIN{res=0;}{if ($0 ~ "[nN]o such file") res=1; if ($0 ~ "[nN]ame or service not known") res=2; if ($0 ~ "[Hh]ost key verification failed") res=3; if ($0 ~ "[pP]ermission denied .*(publickey|password)") res=129;}END{exit res}'
  return ${PIPESTATUS[1]}
}

function httpiptDriver () 
{
  source ${_OUTPUT_DIR}/ipt.cred
  DOWNLOAD_BASE_URL="${download_base_url}"
  AUTH_URL='https://finder.eocloud.eu/resto/api/authidentity'
  DOMAIN_NAME=${domain}
  USERNAME=${user}
  USERPASS=${pwd}

  INPUT_URI="${1#*://}"

  TOKEN=$(curl -s -X POST -F "domainName=${DOMAIN_NAME}" -F "userName=${USERNAME}" -F "userPass=${USERPASS}" "${AUTH_URL}" 2>&1 )
  res=$?
	if [[ "$res" -ne "0" ]]; then
		echo "[ERROR  ][secp][failed] url '$URI' - Preliminary authentication failed"
    		rm -f "$2"
		return $res
	else
		ERROR_MESSAGE=$(echo ${TOKEN} | jq -r .ErrorMessage)
		if [[ "${ERROR_MESSAGE}" != "null" ]] 
		then
			echo "[ERROR  ][secp][failed] url '$URI' - $ERROR_MESSAGE"
			return 1
		else 
			TOKEN_IDENTITY=$(echo ${TOKEN} | jq -r .tokenIdentity)
		fi
	fi
 	URL="${DOWNLOAD_BASE_URL}/$INPUT_URI?token=${TOKEN_IDENTITY}"
	OUTNAME=$(echo $2 | sed -e 's/\(\?.*\)//')
	curl -s -L -o "$OUTNAME"  "$URL" return $?
}

function httpDriver { curlDriver $@; }
function ftpDriver { curlDriver $@; }
function httpsDriver { curlDriver $@; }
function nfsDriver { fileDriver $@; }
function gsiftpDriver { gridftpDriver $@; }

##END DRIVERS DEFINITION

#Download the files one by one
exit_code=0
if $_ABORT_ON_URI_ERROR; then
  ON_ERROR='exit $res'
else
  ON_ERROR='exit_code=1; continue'
fi
filecounter=0
while [[ -n "$_INPUT_URI" ]]; do
  URI="${_INPUT_URI%% *}"
  _INPUT_URI="${_INPUT_URI#* }"
  [[ -z "$URI" ]] && continue
  filecounter=$((filecounter+1))

  echo "[INFO   ][secp][starting] url '$URI' > local '$_OUTPUT_DIR$_OUTPUT_PREFIX'"

  #Search if local file already exists (if so, overwrite or exit)
  _LOCAL_FILE="${URI##*/}"; _LOCAL_FILE="${_LOCAL_FILE%\?*}"; _PRINT_FILENAME="${_LOCAL_FILE}"; _LOCAL_FILE="$_OUTPUT_DIR$_OUTPUT_PREFIX$_LOCAL_FILE"
  if [[ -e "$_LOCAL_FILE" ]]; then
    if $_OVERWRITE; then
      echo "[WARNING][secp][get:file] clearing existing sink '$_LOCAL_FILE'"
      rm -rf $_LOCAL_FILE
    elif $_SKIP_EXISTING_SINKS; then
      echo "[WARNING][secp][get:file] skipping existing sink '$_LOCAL_FILE'"
      echo "[INFO   ][secp][success] url '$URI' > local '$_LOCAL_FILE'"
      $_QUIET ||  echo $_LOCAL_FILE
      #in case of already existing zip files:
      echo "[INFO   ][file $filecounter] downloaded item CURL_MESSAGE: '$CURL_MESSAGE'"
      echo "[INFO   ][file $filecounter] downloaded item PRINT_FNAME: '$_PRINT_FILENAME'"
      if [ -f $_PRINT_FILENAME ]; then
        mv $_PRINT_FILENAME $2
      fi
      continue
    else
      echo "[ERROR  ][secp][failed] sink '$_LOCAL_FILE' already exists"
      res=251
      eval "$ON_ERROR"
    fi
  fi

  #Load credentials stored in the home for the user (if present). Credentials are indexed using the server name and port.
  CRED_INDEX="${URI#*://}"; CRED_INDEX=${CRED_INDEX%%/*}
  loadpass $CRED_INDEX

  #Map URI to driver. Simple mapping is done using URI_PROTOCOL, but more complex mapping can be inplemented (ex. in case of cache protocol, etc...).
  #Drivers shall satisfy the options in the comments above
  [[ "$URI" =~ [a-zA-Z0-9]*:\/\/ ]] || URI="file://$URI"
  URI_DRIVER="${URI%%://*}Driver"

  #Check if driver exists
  if [[ "`type -t $URI_DRIVER`" != "function" ]]; then
            echo "[ERROR  ][secp][failed] protocol ${URI%%://*}:// not supported"
            rm -f $_LOCAL_FILE
            res=1
            eval "$ON_ERROR"
  fi

  #Call the driver
  $URI_DRIVER $URI $_LOCAL_FILE
  res=$?
  if [[ "$res" -eq "1" ]]; then
    if [[ "${URI##*.}" != "gz" ]]; then
      #the file do not exist, try with the .gz extension
      _INPUT_URI="${URI}.gz $_INPUT_URI"
      continue
    else
      echo "[ERROR  ][secp][failed] url '$URI' not found"
      res=251
      eval "$ON_ERROR"
    fi
  fi
  [[ "$res" -ne "0" ]] && eval "$ON_ERROR"

  # Follow HTML (perform also HTML authentication)
  if $_FOLLOW_HTML; then
    if [[ ! -d "$_LOCAL_FILE" && ! -h "$_LOCAL_FILE" && "$(stat -c%s "$_LOCAL_FILE")" -lt 10240 ]]; then
      URI_TO_FOLLOW=
      #check file format by file URI name/path and the first 5 characters
      FB="`head -c 5 $_LOCAL_FILE`"
      if [[ "${URI##*.}" == "uar" ]]; then
        #This is an UAR archive
        URI_TO_FOLLOW="`cat $_LOCAL_FILE`"
      elif [[ "${URI##*/}" =~ ^rdf && "$FB" == "<?xml" ]]; then
        #It seems to be an XML. Try to extract the RDF tags (and do not duplicate the entries)
        URI_TO_FOLLOW=`tr -d '\n' < $_LOCAL_FILE | gawk 'BEGIN{RS=">";p=0}{ if ($1 ~ "<ws:[A-Z]*") { split($2,a,"\""); if (a[1] ~ "^rdf:about") { b=a[2]; sub(/^.*\//,"",b); sub(/\?.*$/,"",b); if (d[b]==0) { d[b]=1; e[p]=a[2]; p++; } } } }END{ for (i=0;i<p;i++) printf e[i]" "; exit p;}'`
      elif [[ "$FB" == "<!DOC" || "$FB" == "<!doc"  || "$FB" == "<html" || "$FB" == "<HTML"  ]]; then
        #It seems to be an HTML. Try to extract the RDF tags (and do not duplicate the entries)
        URI_TO_FOLLOW=`tr -d '\n' < $_LOCAL_FILE | gawk 'BEGIN{RS=">";p=0}{ if ($1" "$2 == "<meta http-equiv=\"refresh\"") { split($3,a,"\""); e[0]="refresh://"a[2]; p=1; exit; } ; if ($1 == "<a") { split($2,a,"\""); if (a[1] == "href=") { b=a[2]; sub(/^.*\//,"",b); sub(/\?.*$/,"",b); if (d[b]==0) { d[b]=1; e[p]=a[2]; p++; } } } }END{ for (i=0;i<p;i++) printf e[i]" "; exit p;}'`
        if [[ "$URI_TO_FOLLOW" =~ ^refresh:// ]]; then
          #this is a meta-refresh
          URI_TO_FOLLOW=${URI_TO_FOLLOW#refresh://}
          ST=${URI_TO_FOLLOW%%;*}
          if [[ "$ST" == "$URI_TO_FOLLOW" ]]; then
            URI_TO_FOLLOW="$URI "
          else
            URI_TO_FOLLOW="${URI_TO_FOLLOW#*;}"
            URI_TO_FOLLOW="${URI_TO_FOLLOW# }"
          fi
          echo "[INFO   ][secp][refresh] got meta-refresh, waiting for $ST seconds"
          sleep $ST
        fi
      elif grep -q '<title>EO SSO</title>' $_LOCAL_FILE; then
        #This seems to be the EO-SSO login page, perform EO-SSO login
        echo "[INFO   ][secp][auth] EO-SSO login page detected. Logging in and re-downloading the file..."

        #Check if the page version and other pre-requisites
        IDP_ADDR="`sed -n -e 's|^[^<]*<a href="\(.*\)/idp/umsso20/admin">Forgot your password?</a>.*$|\1/idp/umsso20/login?null|p' $_LOCAL_FILE`"
        if [[ $? -ne 0 || -z "$IDP_ADDR" ]] || ! grep -q untilbrowserclose $_LOCAL_FILE || ! grep -q oneday $_LOCAL_FILE; then
          echo "[ERROR  ][secp][auth] EO-SSO page invalid. Maybe EO-SSO has been upgraded."
          eval "$ON_ERROR"
        fi
        if [[ $_SESSION_FILE == "/dev/null" || ! -w $_SESSION_FILE ]]; then
          echo "[ERROR  ][secp][auth] You need to enable session file to perform EO-SSO login."
          eval "$ON_ERROR"
        fi

        #Load UM-SSO password (this is mandatory)
        SSO_SERVER=${IDP_ADDR#*://}; SSO_SERVER=${SSO_SERVER%%:*};
        loadpass "$SSO_SERVER" true

        #Try to re-download the file (performing log in...)
        curl --remote-name --keepalive-time 2 -b $_SESSION_FILE -c $_SESSION_FILE -L -k -f -S "$IDP_ADDR" --data "cn=$_SERVER_USERNAME&password=$_SERVER_PASSWORD"'&loginFields=cn@password&loginMethod=umsso&sessionTime=untilbrowserclose&idleTime=oneday'
        res=$?

        #Check if you re-get the EO-SSO page
        if [[ "$res" -ne "0" ]] || [[ "$(stat -c%s "$_LOCAL_FILE")" -lt 10240 && $(grep -c '<title>EO SSO</title>' $_LOCAL_FILE) -eq 1 ]]; then
          echo "[ERROR  ][secp][auth] Failed to login into EO-SSO. Please check your authentication credentials."
          eval "$ON_ERROR"
        fi
      fi
      #Add the URI_TO_FOLLOW to the URI list
      if [[ -n "$URI_TO_FOLLOW" ]]; then
        echo "[INFO   ][secp][success] got URIs '$URI_TO_FOLLOW'"
        _INPUT_URI="$URI_TO_FOLLOW$_INPUT_URI"
        rm -f $_LOCAL_FILE
        continue
      fi
    fi
  fi

  # Uncompress the file
  if $_UNCOMPRESS; then
    case ${_LOCAL_FILE##*.} in
      gz)
        echo "[INFO   ][secp][unpack:gz] got url as '${_LOCAL_FILE##*/}' - unpacking"
        _LOCAL_FILE=${_LOCAL_FILE%.gz}
        gunzip -c ${_LOCAL_FILE}.gz > ${_LOCAL_FILE}
        res=$?
        if [[ "$res" -ne "0" ]]; then
          echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE' failed (gunzip returned $res)"
          eval "$ON_ERROR"
        fi
        rm -f ${_LOCAL_FILE}.gz
      ;;
      tgz)
        echo "[INFO   ][secp][unpack:tgz] got url as '${_LOCAL_FILE##*/}' - unpacking"
        _LOCAL_FILE="${_LOCAL_FILE%.tgz}"
        tar xz -C $_OUTPUT_DIR -f $_LOCAL_FILE.tgz
        res=$?
        if [[ "$res" -ne "0" ]]; then
          echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE' failed (tar returned $res)"
          eval "$ON_ERROR"
        fi
        rm -f $_LOCAL_FILE.tgz
      ;;
      zip)
        echo "[INFO   ][secp][unpack:zip] got url as '${_LOCAL_FILE##*/}' - unpacking"
        _LOCAL_FILE=${_LOCAL_FILE%.zip}
        mkdir $_LOCAL_FILE
        unzip -qq -o $_LOCAL_FILE.zip -d $_LOCAL_FILE
        res=$?
        if [[ "$res" -ne "0" ]]; then
          rmdir $_LOCAL_FILE
          echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE.zip' failed (unzip returned $res)"
          eval "$ON_ERROR"
        fi
        rm -f $_LOCAL_FILE.zip
        case `find $_LOCAL_FILE -mindepth 1 -maxdepth 1 | wc -l` in
          0) echo "[ERROR  ][secp][failed] unpaking '$_LOCAL_FILE.zip' failed (empty archive)"
             eval "$ON_ERROR"
          ;;
          1) EXTRACTED_LFILE="`find $_LOCAL_FILE -mindepth 1 -maxdepth 1`"
             mv $EXTRACTED_LFILE ${_LOCAL_FILE%/*}/tmpfilename$$
             rmdir $_LOCAL_FILE &>/dev/null
             mv ${_LOCAL_FILE%/*}/tmpfilename$$ "${_LOCAL_FILE%/*}/${EXTRACTED_LFILE##*/}"
             _LOCAL_FILE="${_LOCAL_FILE%/*}/${EXTRACTED_LFILE##*/}"
          ;;
        esac
      ;;
     *)
       ;;
    esac
  fi

  #Compress the file
  if $_COMPRESS_OUTPUT; then
    if [[ -h "$_LOCAL_FILE" ]]; then
      FILE_TO_PACK="`readlink -f $_LOCAL_FILE`"
    else
      FILE_TO_PACK="$_LOCAL_FILE"
    fi
    if [[ -d "$FILE_TO_PACK" ]]; then
      FILE_TO_PACK="${FILE_TO_PACK%/}"
      echo "[INFO   ][secp][pack:tgz] got url as '${_LOCAL_FILE##*/}' - packing"
      tar cz -C ${FILE_TO_PACK%/*} -f $_LOCAL_FILE.tgz ${FILE_TO_PACK##*/}
      res=$?
     if [[ "$res" -ne "0" ]]; then
        echo "[ERROR  ][secp][failed] paking '$_LOCAL_FILE' failed (tar returned $res)"
        eval "$ON_ERROR"
      fi
      rm -rf $_LOCAL_FILE
      _LOCAL_FILE="$_LOCAL_FILE.tgz"
    else
      if [[ "${FILE_TO_PACK##*.}" == "gz" || "${FILE_TO_PACK##*.}" == "tgz" || "${FILE_TO_PACK##*.}" == "zip" ]]; then
        echo "[INFO   ][secp][pack:${_LOCAL_FILE##*.}] got url as '${_LOCAL_FILE##*/}' - already packed"
      else
        echo "[INFO   ][secp][pack:gz] got url as '${_LOCAL_FILE##*/}' - packing"
        gzip -c $FILE_TO_PACK > $_LOCAL_FILE.gz
        res=$?
        if [[ "$res" -ne "0" ]]; then
          echo "[ERROR  ][secp][failed] paking '$_LOCAL_FILE' failed (gzip returned $res)"
          eval "$ON_ERROR"
        fi
        rm -f $_LOCAL_FILE
        _LOCAL_FILE="$_LOCAL_FILE.gz"
        [[ ${myfile/#*zip/ZIP} == "ZIP" ]] && secp_flags=""
      fi
    fi
  fi
  echo "[INFO   ][secp][success] url '$URI' > local '$_LOCAL_FILE'"
  # Echo the file to stdout
  $_QUIET || echo $_LOCAL_FILE
  #in case of new zip files:
  echo "[INFO   ][file $filecounter] Saved to filename '$_PRINT_FILENAME'"
  if [ -f $_PRINT_FILENAME ]; then
    mv $_PRINT_FILENAME $_OUTPUT_DIR
  fi
done

exit $exit_code

